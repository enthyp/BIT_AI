{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome!\n",
    "\n",
    "Today we're going to present last-time's topic - **Linear Regression** in a more generalized way. You'll see that this algorithm doesn't change a lot, when we want to fit our model to more features than just one!\n",
    "\n",
    "This will be an occasion to introduce and tackle some problems faced by data scientists on a daily basis, such as:\n",
    "- data normalization\n",
    "- regularization of the cost function\n",
    "- overfitting\n",
    "- dividing the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, fixed\n",
    "import ipywidgets as widgets\n",
    "import solutions\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem\n",
    "\n",
    "## Previously\n",
    "\n",
    "### $$\\hat{y} = h_W(x) = w_0 + w_1x$$ \n",
    "\n",
    "## Today\n",
    "\n",
    "### $$\\hat{y} = h_W(x_1, x_2, ..., x_k) = w_0 + w_1x_1+ w_2x_2+ w_3x_3+ ... + w_kx_k = w_0 + \\sum_{i=1}^k w_i x_i$$ \n",
    "\n",
    "## or:\n",
    "\n",
    "### $W$ - vector of coefficients or 'weights'\n",
    "\n",
    "$$ W = [w_0, w_1, ..., w_k]$$\n",
    "\n",
    "### $X^{(i)}$ - vector of  features of an i-th test case\n",
    "$$ X^{(i)} = [x^{(i)}_0, x^{(i)}_1, ... x^{(i)}_k] $$\n",
    "... where $x_0$ = 1, so that\n",
    "$$h_W(X^{(i)}) = \\sum_{j=0}^k w_j x_j = W * X^{(i)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "[[ 1.  1.  2.  3.]\n",
      " [ 1.  4.  5.  6.]]\n"
     ]
    }
   ],
   "source": [
    "def add_bias_feature(X):\n",
    "       return np.c_[np.ones(len(X)), X]\n",
    "\n",
    "X = np.array([[1,2,3], [4,5,6]])\n",
    "print(X)\n",
    "print(add_bias_feature(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypotheses(W, X):\n",
    "    # W: a vector of weights\n",
    "    # X: a list of feature vectors of some objects (so effectively a matrix)\n",
    "    # return a vector of hypotheses for *all* x-s \n",
    "    return(np.dot(X, W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses = solutions.hypotheses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost function\n",
    "\n",
    "### $$L = \\frac{1}{2N}\\sum_{i=0}^N(h_W(x^{(i)}) - y^{(i)})^2 $$\n",
    "## Previously\n",
    "\n",
    "### $$L(w_0, w_1) = \\frac{1}{2N}\\sum_{i=0}^N(w_0 + w_1x^{(i)} - y^{(i)})^2 $$\n",
    "\n",
    "## Today\n",
    "\n",
    "### $$L(w_0, w_1, ... w_n) = L(W) = \\frac{1}{2N}\\sum_{i=0}^N(\\sum_{j=0}^k w_j x^{(i)}_j - y^{(i)})^2 = \\frac{1}{2N}\\sum_{i=0}^N (h_W(x^{(i)}) - y^{(i)})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(W, X, Y):\n",
    "    # W: a vector of weights\n",
    "    # X: a list of feature vectors of some objects (so effectively a matrix)\n",
    "    # Y: a vector of our values\n",
    "    # return cost (a scalar)\n",
    "    differences = hypotheses(W, X) - Y\n",
    "    return(1 / (2 * X.shape[0]) * np.dot(differences, differences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = solutions.cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent\n",
    "\n",
    "For every iteration:\n",
    "* calculate partial derivatives of cost function with respect to every element of W:\n",
    "\n",
    "$$\\epsilon_j = \\frac{\\partial}{\\partial w_j}L(W) = \\frac{1}{N} \\sum_{i=1}^N(h_W(x^{(i)}) - y^{(i)})x_j^{(i)}$$\n",
    "\n",
    "* **simultaneously** update every element of W:\n",
    "\n",
    "$$w_j = w_j - \\alpha \\epsilon_j$$ \n",
    "\n",
    "Where $\\alpha$ is our learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_step(W, X, Y, learning_rate=0.1):\n",
    "    # W: a vector of weights\n",
    "    # X: a list of feature vectors of some objects (so effectively a matrix)\n",
    "    # Y: a vector of our values\n",
    "    # return a vector of new values of W\n",
    "    differences = hypotheses(W, X) - Y\n",
    "    eps = 1 / X.shape[0] * np.dot(np.transpose(X), differences)\n",
    "    return(W - alpha * eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_step = solutions.gradient_step"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
